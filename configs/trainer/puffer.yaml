_target_: rl.pufferlib.trainer.PufferTrainer

defaults:
  - trainer

total_timesteps: 50_000_000_000

scheduler: cosine
precision: float32

clip_reward: false

ppo:
  gamma: 0.97
  gae_lambda: 0.9
  clip_vloss: true
  vf_clip_coef: 0.1
  vf_coef: 0.5
  ent_coef: 0.002

p3o:
  enabled: False
  horizon: 128
  puf: 0.0

optimizer:
  type: muon # adam
  beta1: 0.9
  beta2: 0.999
  eps: 1e-5
  learning_rate: 0.0006

diayn:
  enabled: False
  archive: 8
  loss_coef: 1.0
  coef: 0.1

e3b:
  enabled: False
  coef: 0.01
  norm: 0.001
  lambda_: 10.0

l2_reg:
  loss_coef: 0
  init_loss_coef: 0

average_reward:
  enabled: false
  alpha: 0.01

max_grad_norm: 0.5
norm_adv: true
clip_vloss: true
target_kl: null
anneal_lr: false

zero_copy: true
require_contiguous_env_ids: false
verbose: true

batch_size: 262144
minibatch_size: 4096
max_minibatch_size: 16384
bptt_horizon: 16
update_epochs: 1

cpu_offload: false
compile: false
compile_mode: reduce-overhead

forward_pass_minibatch_target_size: 2048
async_factor: 2

stats:
  overview:
    episode/reward.mean: episode_reward
  step: train/agent_step

kickstart:
  teacher_uri: null
  action_loss_coef: 1
  value_loss_coef: 1
  kickstart_steps: 50_000_000
  additional_teachers:
    # - teacher_uri: wandb://run/mettabox_cogeval_defaults_lowent_initialized0005:v100
    #   action_loss_coef: 1
    #   value_loss_coef: 1
    #  - teacher_uri: wandb://run/mettabox_cogeval_defaults_lowent_initialized0005:v95
    #    action_loss_coef: 1
    #    value_loss_coef: 1
