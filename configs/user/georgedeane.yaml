# @package __global__

defaults:
  - override /agent: simple
  - override /eval: eval
  - _self_

trainer:
  env: /env/mettagrid/walkaround
  evaluate_interval: 2

# policy: wandb://run/b.daveey.train.maze.sm.dr.warm.0
# baselines: wandb://run/b.daveey.train.maze.sm.11x11.0

# policy_uri: wandb://run/b.daveey.sm.train.er.new.0
# policy_uri: wandb://run/daveey.ar.cards.1
# policy_uri: wandb://run/b.daveey.t.32.instant
#policy_uri: ${trained_policy_uri}
policy_uri: wandb://run/terrain_multienv_3_single_agent
# b.georgedeane.terrain_multienv
# policy_uri: wandb://run/terrain_training_multienv_april18
# policy_uri: wandb://run/b.daphne.terrain_multienv_april18


# Infinite_cooldown models:
# navigation_infinite_cooldown_sweep_2g_.r.0 - ok
# navigation_infinite_cooldown_sweep_2g.r.1 - 9 reward after 400 timesteps
# navigation_infinite_cooldown_high_ent_no_initial_heart - ~9, current best
# navigation_poisson_train_sampling5 breaks in infinite_cooldown, works great in poisson with short distance
# navigation_poisson_train_sampling5
# npc_policy_uri: ${trained_policy_uri}
# eval_db_uri: wandb://artifacts/navigation_constrained_actions
eval_db_uri: wandb://artifacts/training_nav_db_slava

analyzer:
  # eval_stats_uri: ${run_dir}/eval_stats
  policy_uri: ${..policy_uri}
  analysis:
    metrics:
      - metric: episode_reward
  #output_path: evalresults/training_navigation.html
  output_path: s3://softmax-public/policydash/dashboard_navigation_db3.html

eval:
  num_envs: 2
  num_episodes: 2
  max_time_s: 600

  # policy_uri: ${..policy_uri}
  # npc_policy_uri: ${..npc_policy_uri}
  # eval_db_uri: ${..eval_db_uri} #file://daphne/sweep_stats
  env: env/mettagrid/slava/48x61
  # eval_db_uri: wandb://artifacts/navigation_db2
  # ${..eval_db_uri} 

  
  # navigation/training/varied_terrain_all_dense_small



run_id: 103
run: ${oc.env:USER}.local.${run_id}
trained_policy_uri: ${run_dir}/checkpoints

sweep_params: "sweep/fast"
sweep_name: "${oc.env:USER}.local.sweep.${run_id}"
seed: null
