# @package __global__

#POLICIES TO EVALUATE

#b.daphne.navigation_varied_obstacle_shapes_pretrained.r.1
#b.daphne.navigation_varied_obstacle_shapes.r.0
#navigation_poisson_sparser.r.2
#navigation_infinite_cooldown_sparser_pretrained.r.0
#navigation_infinite_cooldown_sparser.r.0
#navigation_poisson_sparser_pretrained.r.6

defaults:
  - /hardware: macbook
  - _self_

# Basic config variables
run: axel.test_train
data_dir: ./train_dir
run_dir: ${data_dir}/${run}

trainer:
  env: /env/mettagrid/simple
  evaluate_interval: 10
  num_workers: 1
  forward_pass_minibatch_target_size: 2
  async_factor: 1
  batch_size: 1024
  minibatch_size: 1024

# policy: wandb://run/b.daveey.train.maze.sm.dr.warm.0
# baselines: wandb://run/b.daveey.train.maze.sm.11x11.0

# policy_uri: wandb://run/b.daveey.sm.train.er.new.0
# policy_uri: wandb://run/daveey.ar.cards.1
# policy_uri: wandb://run/b.daveey.t.32.instant

# policy_uri: wandb://run/b.daphne.terrain_multienv_lessactions:v40
# policy_uri: wandb://run/terrain_multienv_3_single_agent:v63
policy_uri: wandb://run/b.daphne.terrain_prioritized_styles_pretrained_r:v1

# npc_policy_uri: ${trained_policy_uri}


# dashboard:
#   # output_path: s3://softmax-public/policydash/dashboard.html
#  # output_path: navigation_results/navigation.html
#   output_path: navigation_results/navigation.html

# sim:
#   env: /env/mettagrid/multiagent/evals/spine_maze_extended



wandb:
  enabled: true
  track: true
  checkpoint_interval: 1

run_id: test_train
trained_policy_uri: ${run_dir}/checkpoints

# Optional: Set a fixed seed for reproducibility
seed: 42

# Hardware-specific settings (these will be merged with macbook.yaml)
device: cpu
vectorization: serial
torch_deterministic: true

sweep_params: "sweep/fast"
sweep_name: "${oc.env:USER}.local.sweep.${run_id}"
